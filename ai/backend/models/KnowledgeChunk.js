const fs = require("fs");
const mammoth = require("mammoth");
const path = require("path");
const dotenv = require("dotenv");
const { CloudClient } = require("chromadb");
const fetch = require("node-fetch");
const { HfInference } = require("@huggingface/inference");

// ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file .docx
const DOCS_DIR = "C:/Code/ProjCT/knowledgeBase/docx";

// ThÃªm Ä‘Æ°á»ng dáº«n thÆ° má»¥c chunks
const CHUNKS_DIR = "C:/Code/ProjCT/knowledgeBase/chunks";

// Táº£i biáº¿n mÃ´i trÆ°á»ng tá»« .env
dotenv.config({ path: path.resolve(__dirname, "../.env") });

// Sá»­a cÃ¡ch khá»Ÿi táº¡o client
const inference = new HfInference(process.env.HF_TOKEN);

// Khá»Ÿi táº¡o ChromaDB client vÃ  collection
const chromaClient = new CloudClient({
  apiKey: process.env.CHROMA_API_KEY,
  tenant: process.env.CHROMA_TENANT,
  database: process.env.CHROMA_DATABASE
});

let collection;
async function initChroma() {
  collection = await chromaClient.getOrCreateCollection({
    name: "knowledge_chunks",
    metadata: { "description": "ESH knowledge base chunks" }
  });
}

// ThÃªm hÃ m delay helper
const delay = ms => new Promise(resolve => setTimeout(resolve, ms));

// HÃ m retry vá»›i backoff
async function retryWithBackoff(fn, maxRetries = 3) {
  let retries = 0;
  while (retries < maxRetries) {
    try {
      return await fn();
    } catch (error) {
      retries++;
      if (retries === maxRetries) throw error;
      
      // TÄƒng thá»i gian chá» theo cáº¥p sá»‘ nhÃ¢n
      const waitTime = Math.min(1000 * Math.pow(2, retries), 10000);
      console.log(`Retry ${retries}/${maxRetries} after ${waitTime}ms...`);
      await delay(waitTime);
    }
  }
}

// Helper function to temporarily suppress logs
const suppressLogsTemporarily = async (fn) => {
  const oldLog = console.log;
  console.log = () => {};
  try {
    return await fn();
  } finally {
    console.log = oldLog;
  }
};

// HÃ m gá»i Hugging Face API Ä‘á»ƒ láº¥y embedding
async function getEmbedding(text) {
  if (!text || typeof text !== 'string') {
    throw new Error('Input text is required and must be a string');
  }

  const cleanText = text.trim().replace(/\s+/g, ' ').slice(0, 2000);

  try {
    const output = await retryWithBackoff(async () => {
      // KhÃ´ng táº¯t log toÃ n cá»¥c ná»¯a â€“ chá»‰ log lá»—i náº¿u cÃ³
      const result = await inference.featureExtraction({
        model: "sentence-transformers/all-MiniLM-L6-v2",
        inputs: cleanText,
        options: {
          wait_for_model: true,
          use_cache: true
        }
      });

      if (!result || !Array.isArray(result)) {
        throw new Error('Invalid embedding format received');
      }

      return result;
    });

    return output;
  } catch (err) {
    console.error('âŒ Embedding error details:', {
      message: err.message,
      textLength: text?.length,
      textPreview: text?.slice(0, 100)
    });
    throw err;
  }
}

// HÃ m chuáº©n hÃ³a tÃªn file (loáº¡i bá» .docx)
function normalizeFileName(file) {
  return file.replace(/\.docx$/i, "");
}

// HÃ m chia nhá» vÄƒn báº£n theo cÃ¢u vÃ  sá»‘ lÆ°á»£ng tá»«
function chunkTextBySentence(text, minWords = 100, maxWords = 150) {
  // TÃ¡ch thÃ nh cÃ¢u dá»±a trÃªn dáº¥u cháº¥m, há»i, cáº£m thÃ¡n
  const sentences = text
    .replace(/\r\n/g, ' ')
    .replace(/\n/g, ' ')
    .replace(/\s+/g, ' ')
    .replace(/\t/g, ' ')
    .trim()
    .split(/(?<=[.!?])\s+/);

  const chunks = [];
  let currentChunk = [];
  let wordCount = 0;

  for (const sentence of sentences) {
    const words = sentence.split(' ');
    currentChunk.push(sentence);
    wordCount += words.length;

    if (wordCount >= minWords) {
      chunks.push(currentChunk.join(' ').trim());
      currentChunk = [];
      wordCount = 0;
    }
  }
  // ThÃªm pháº§n cÃ²n láº¡i náº¿u cÃ³
  if (currentChunk.length > 0) {
    chunks.push(currentChunk.join(' ').trim());
  }
  return chunks;
}

// HÃ m kiá»ƒm tra file Ä‘Ã£ chunk chÆ°a báº±ng ID
async function isFileChunked(file) {
  await initChroma();
  const fileId = normalizeFileName(file);
  const expectedChunkId = `${fileId}-chunk-0`;

  try {
    const result = await collection.get({ ids: [expectedChunkId] });
    return result.ids.includes(expectedChunkId);
  } catch (err) {
    return false;
  }
}

// HÃ m Ä‘á»c chunks tá»« file JSON
async function loadChunksFromJson(jsonPath) {
  try {
    const data = JSON.parse(fs.readFileSync(jsonPath, 'utf8'));
    return {
      filename: data.filename,
      chunks: data.chunks
    };
  } catch (err) {
    console.error(`Error reading chunks from ${jsonPath}:`, err);
    return null;
  }
}

// HÃ m lÆ°u chunk vÃ o ChromaDB
async function extractAndSaveDocs() {
  try {
    await initChroma();

    const files = fs.readdirSync(DOCS_DIR)
      .filter(file => file.endsWith(".docx") && !file.startsWith("~$"));

    let processedCount = 0;

    for (const file of files) {
      const fileId = normalizeFileName(file);
      if (await isFileChunked(file)) {
        console.log(`â© [ÄÃƒ Xá»¬ LÃ] File ${file} Ä‘Ã£ Ä‘Æ°á»£c chunk, bá» qua.`);
        continue;
      }
      try {
        console.log(`\nğŸ“ Processing file: ${file}`);
        const filePath = path.join(DOCS_DIR, file);

        const result = await mammoth.extractRawText({ path: filePath });
        if (!result.value) {
          throw new Error('Could not read file content');
        }

        const chunks = chunkTextBySentence(result.value, 100, 150);
        console.log(`ğŸ”„ Created ${chunks.length} chunks from file ${file}`);

        for (let i = 0; i < chunks.length; i += 10) {
          const batchChunks = chunks.slice(i, i + 10);
          const ids = batchChunks.map((_, index) => `${fileId}-chunk-${i + index}`);

          // ThÃªm log Ä‘á»ƒ debug chunk nÃ o bá»‹ treo
          console.log(`ğŸ” Äang láº¥y embedding cho cÃ¡c chunk:`, ids);

          const embeddings = await Promise.all(
            batchChunks.map(async (chunk, idx) => {
              try {
                return await getEmbedding(chunk);
              } catch (err) {
                console.error(`âŒ Lá»—i embedding chunk ${ids[idx]}:`, err);
                throw err;
              }
            })
          );

          await collection.add({
            ids: ids,
            embeddings: embeddings,
            metadatas: batchChunks.map(chunk => ({
              file: fileId,
              chunkSize: chunk.split(' ').length
            })),
            documents: batchChunks
          });

          console.log(`âœ… Saved chunks ${i + 1} to ${i + batchChunks.length}`);
          await delay(1000);
        }
        processedCount++;
        console.log(`\nğŸ¯ ÄÃ£ hoÃ n táº¥t xá»­ lÃ½ file: ${file}`);
        console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);
      } catch (fileError) {
        console.error(`âŒ Error processing file ${file}:`, fileError);
        continue;
      }
    }

    console.log(`ğŸ‰ ÄÃ£ hoÃ n táº¥t chunkin ${processedCount} file!`);
  } catch (err) {
    console.error('âŒ Critical error:', err);
  }

  try {
    // ThÃªm xá»­ lÃ½ chunks tá»« Python
    const jsonFiles = fs.readdirSync(CHUNKS_DIR)
      .filter(file => file.endsWith("_chunks.json"));

    for (const jsonFile of jsonFiles) {
      const fileId = jsonFile.replace(/_chunks\.json$/i, "");
      
      if (await isFileChunked(fileId)) {
        console.log(`â© [ÄÃƒ Xá»¬ LÃ] File ${fileId} Ä‘Ã£ Ä‘Æ°á»£c chunk, bá» qua.`);
        continue;
      }

      try {
        console.log(`\nğŸ“ Processing chunks from: ${jsonFile}`);
        const chunksData = await loadChunksFromJson(path.join(CHUNKS_DIR, jsonFile));
        
        if (!chunksData) continue;

        const { chunks } = chunksData;
        console.log(`ğŸ”„ Loaded ${chunks.length} chunks from ${jsonFile}`);

        // Xá»­ lÃ½ tá»«ng batch chunks
        for (let i = 0; i < chunks.length; i += 10) {
          const batchChunks = chunks.slice(i, i + 10);
          const ids = batchChunks.map((_, index) => `${fileId}-chunk-${i + index}`);

          console.log(`\nğŸ” Batch ${Math.floor(i/10) + 1}/${Math.ceil(chunks.length/10)}:`);
          console.log(`âŒ› Äang láº¥y embedding cho cÃ¡c chunk:`, ids);

          const embeddings = await Promise.all(
            batchChunks.map(async (chunk, idx) => {
              try {
                return await getEmbedding(chunk);
              } catch (err) {
                console.error(`âŒ Lá»—i embedding chunk ${ids[idx]}:`, err);
                throw err;
              }
            })
          );

          await collection.add({
            ids: ids,
            embeddings: embeddings,
            metadatas: batchChunks.map(chunk => ({
              file: fileId,
              chunkSize: chunk.split(' ').length
            })),
            documents: batchChunks
          });

          console.log(`âœ… ÄÃ£ lÆ°u batch ${Math.floor(i/10) + 1}: chunks ${i + 1} Ä‘áº¿n ${i + batchChunks.length}`);
          await delay(1000);
        }

        console.log(`\nğŸ¯ ÄÃ£ hoÃ n táº¥t xá»­ lÃ½ file: ${fileId}`);
        console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);

      } catch (error) {
        console.error(`âŒ Error processing ${jsonFile}:`, error);
        continue;
      }
    }

  } catch (err) {
    console.error('âŒ Critical error:', err);
  }
}

// HÃ m tÃ¬m kiáº¿m chunk tá»« ChromaDB
async function searchSimilarChunks(query, limit = 3) {
  try {
    const queryEmbedding = await getEmbedding(query);

    const results = await collection.query({
      queryEmbeddings: [queryEmbedding],
      nResults: limit
    });

    // Log káº¿t quáº£ truy váº¥n
    console.log("ğŸ” Query:", query);
    console.log("ğŸ“¥ Returned:", results.documents[0]?.length || 0, "chunks");
    console.log("ğŸ“ Distances:", results.distances?.[0]);

    // Náº¿u khÃ´ng cÃ³ chunk phÃ¹ há»£p, tráº£ vá» fallback
    if (!results.documents[0] || results.documents[0].length === 0) {
      return [
        "TÃ´i xin lá»—i, tÃ´i khÃ´ng cÃ³ thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong tÃ i liá»‡u tuyá»ƒn sinh."
      ];
    }

    return results.documents[0]; // Tráº£ vá» cÃ¡c chunk phÃ¹ há»£p

  } catch (err) {
    console.error('Search error:', err);
    return [
      "TÃ´i xin lá»—i, tÃ´i khÃ´ng cÃ³ thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong tÃ i liá»‡u tuyá»ƒn sinh."
    ];
  }
}

// Log ra Ä‘á»ƒ kiá»ƒm tra biáº¿n mÃ´i trÆ°á»ng Ä‘Ã£ load chÆ°a
console.log("HF_TOKEN:", process.env.HF_TOKEN?.slice(0, 10) + "...");

// Cháº¡y script
async function main() {
  await extractAndSaveDocs();
}

if (require.main === module) {
  main();
}

// Export cÃ¡c hÃ m cáº§n thiáº¿t
module.exports = {
  extractAndSaveDocs,
  searchSimilarChunks
};